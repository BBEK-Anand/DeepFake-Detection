{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f85ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f65134",
   "metadata": {},
   "source": [
    "# Step:- 2 | Modelling the componets(dataset and Architecture)\n",
    "## 1st Dataset component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f22ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# this dataset class should take a path, which is we have fixed using set_default_config of parameter train_data_src and valid_data_src\n",
    "# in our case real and fake images are in different folders, so the Dataset class should be made in such a way.\n",
    "# also \n",
    "class DS01(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.Resize((128,128)),  # Resize images \n",
    "                            transforms.ToTensor()        # Convert images to tensor\n",
    "                        ])\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Map directories with 'real' or 'fake' in their name to corresponding labels\n",
    "        self.class_to_idx = {'real': 1, 'fake': 0}  # 1 for real, 0 for fake\n",
    "        \n",
    "        # Traverse through subdirectories\n",
    "        for subdir in os.listdir(root_dir):\n",
    "            class_name = None\n",
    "            \n",
    "            # Identify if subdir is 'real' or 'fake' by checking the directory name\n",
    "            if 'real' in subdir.lower():\n",
    "                class_name = 'real'\n",
    "            elif 'fake' in subdir.lower():\n",
    "                class_name = 'fake'\n",
    "            \n",
    "            if class_name:\n",
    "                class_dir = os.path.join(root_dir, subdir)\n",
    "                if os.path.isdir(class_dir):\n",
    "                    for img_name in os.listdir(class_dir):\n",
    "                        img_path = os.path.join(class_dir, img_name)\n",
    "                        # Only load .jpg files\n",
    "                        if img_name.lower().endswith('.jpg'):\n",
    "                            self.image_paths.append(img_path)\n",
    "                            self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Open image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83489b1",
   "metadata": {},
   "source": [
    "### checking input size for the model that is returned from DS01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006b2093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 128, 128]), tensor(0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an instance of DS01 for testing\n",
    "dataset = DS01(root_dir=\"../DataSet/real_vs_fake/real_vs_fake/train/\")\n",
    "# DataLoader for batching\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dl = iter(dataloader)\n",
    "X,y = next(dl)\n",
    "# X and y are the input and target of first batch \n",
    "# so we take 1 input and its target to check their sizes\n",
    "X[0].shape,y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a4abd",
   "metadata": {},
   "source": [
    "***Here [3, 128, 128] is the input size*** so model should take input size of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3859f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 128, 128]             224\n",
      "       BatchNorm2d-2          [-1, 8, 128, 128]              16\n",
      "         MaxPool2d-3            [-1, 8, 64, 64]               0\n",
      "            Linear-4                   [-1, 16]         524,304\n",
      "           Dropout-5                   [-1, 16]               0\n",
      "            Linear-6                    [-1, 1]              17\n",
      "================================================================\n",
      "Total params: 524,561\n",
      "Trainable params: 524,561\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 2.25\n",
      "Params size (MB): 2.00\n",
      "Estimated Total Size (MB): 4.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# A CNN Model\n",
    "class testCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n",
    "        self.fc1 = nn.Linear(8*64*64, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = x.to(next(self.parameters()).device) # to assign the input to the same device,\n",
    "#         torchsummary.summary sets model to gpu but for input it does not\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature maps\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "model1 = testCNN()\n",
    "summary(model1,input_size=(3,128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5873c",
   "metadata": {},
   "source": [
    "***From the avobe Total params is 524,561.*** \n",
    "### Now we have to  check the compactibility of model and dataset components\n",
    "Call `test_mods` if your default_config is set ( using `set_default_config` that i have done in `training.ipynb` file) then you only have to set the parametrs `model` and `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f90fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file is saved at internal/Test/test_c.json\n",
      "History will be saved at internal/Test/test_h.csv\n",
      "Weights will be saved at internal/Test/test_w.pth\n",
      "Data loaders are successfully created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|█| 3125/3125 [1:04:24<00:00,  1.24s/it, accuracy=0.5, loss=0.69\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6957, Train Accuracy: 0.50, Val Loss: 0.6931, Val Accuracy: 0.50\n",
      "Best Model Weights Updated: Epoch 1 - Val Loss: 0.6931499324256568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|█| 3125/3125 [04:18<00:00, 12.07it/s, accuracy=0.501, loss=0.69\n",
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.6932, Train Accuracy: 0.50, Val Loss: 0.6932, Val Accuracy: 0.50\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from PyTorchLabFlow import test_mods\n",
    "P = test_mods(dataset=DS01,model=model1,prepare=True)\n",
    "P.train(num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588258fd",
   "metadata": {},
   "source": [
    "If training successfully started then model and dataset classes are compactible to eachother and if not then you should check according to error.\n",
    "\n",
    "If everything goes wel we should move for the next steps\n",
    "\n",
    "### Place the components at the respective files\n",
    "copy the whole code of model class, in our case it is\n",
    "```python\n",
    "class testCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n",
    "        self.fc1 = nn.Linear(8*64*64, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = x.to(next(self.parameters()).device) # to assign the input to the same device,\n",
    "#         torchsummary.summary sets model to gpu but for input it does not\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature maps\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "```\n",
    "copy thye avobe and paste inside `Libs/models.py`\n",
    "and copy the dataset class \n",
    "```python\n",
    "class DS01(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.Resize((128,128)),  # Resize images \n",
    "                            transforms.ToTensor()        # Convert images to tensor\n",
    "                        ])\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Map directories with 'real' or 'fake' in their name to corresponding labels\n",
    "        self.class_to_idx = {'real': 1, 'fake': 0}  # 1 for real, 0 for fake\n",
    "        \n",
    "        # Traverse through subdirectories\n",
    "        for subdir in os.listdir(root_dir):\n",
    "            class_name = None\n",
    "            \n",
    "            # Identify if subdir is 'real' or 'fake' by checking the directory name\n",
    "            if 'real' in subdir.lower():\n",
    "                class_name = 'real'\n",
    "            elif 'fake' in subdir.lower():\n",
    "                class_name = 'fake'\n",
    "            \n",
    "            if class_name:\n",
    "                class_dir = os.path.join(root_dir, subdir)\n",
    "                if os.path.isdir(class_dir):\n",
    "                    for img_name in os.listdir(class_dir):\n",
    "                        img_path = os.path.join(class_dir, img_name)\n",
    "                        # Only load .jpg files\n",
    "                        if img_name.lower().endswith('.jpg'):\n",
    "                            self.image_paths.append(img_path)\n",
    "                            self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Open image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "```\n",
    "and paste it inside `Libs/datasets.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a45f89",
   "metadata": {},
   "source": [
    "## Now time to go to `training.ipynb`\n",
    "# check there for further proccesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbebc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
